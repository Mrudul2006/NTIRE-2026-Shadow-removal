{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Environment Setup\n",
        "!pip install timm lpips opencv-python"
      ],
      "metadata": {
        "id": "tW89OqeTu-yd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11eb81d3-e416-4283-f567-6f98f062a625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2026.1.4)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = f\"/content/drive/MyDrive/\"\n",
        "\n",
        "print(f\"Checking paths in: {base_path} ...\")\n",
        "if os.path.exists(base_path):\n",
        "    print(\"‚úÖ Project Folder Found!\")\n",
        "    print(\"Contents:\", os.listdir(base_path))\n",
        "else:\n",
        "    print(f\"‚ùå Error: Folder not found.\")"
      ],
      "metadata": {
        "id": "B-ik8t_IwhPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbd1e64-d994-49df-d4df-c93a63f31635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking paths in: /content/drive/MyDrive/ ...\n",
            "‚úÖ Project Folder Found!\n",
            "Contents: ['Narendra_Avasthi_Latest_14e_Problems_in_Physicsal_Chemistry_Book.pdf', 'Generated PDF from Image Files.pdf', 'FCP Laboratory Journal-1 (Final).docx', 'EC-C U24EV040 HITESH BEHANI.gdoc', 'Table of Contents.pdf', 'bookswap', 'bookswap 2', '173761784656690659225098794412.jpg', 'Untitled document.gdoc', '17432274405556073219855521790897.jpg', 'Presentation 15 (2).pptx', '174720725515982815960953652529.jpg', 'Colab Notebooks', 'Task 2.docx', 'ecell.pptx', 'Mrudul-Mittal-FlowCV-Resume-20250805.pdf', 'Mrudul-Mittal-FlowCV-Resume-20250805 (1).pdf', 'PEEBM FINAL.gdoc', 'LIST OF TABLES.gdoc', 'peebm report.gdoc', 'aadharcard.pdf', 'Delhi!!!', 'Shadow removal', 'Colab_26dB_Results', 'genesis', 'Recording_20260113_232609.mp4', 'Sequence 01_7.mp4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Drive (Data & Weights ke liye)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
     
        "import os\n",
        "%cd /content\n",
      
        "!git clone https://github.com/Yuyok/FusionShadowRemoval-main\n",
        "REPO_NAME = \"FusionShadowRemoval-main\"\n",
        "%cd /content/{REPO_NAME}"
      ],
      "metadata": {
        "id": "L6yzOrQexgFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8335959-6ad2-4a4f-aa25-0d5d31115a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content\n",
            "Cloning into 'FusionShadowRemoval-main'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 116 (delta 2), reused 116 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (116/116), 3.97 MiB | 4.48 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "/content/FusionShadowRemoval-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia pytorch-lightning timm==0.6.13 lpips opencv-python pyyaml omegaconf"
      ],
      "metadata": {
        "id": "2QyLKb0W2MDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47aedb5-2ac1-49e2-810a-f8346603e64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.8.2-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting timm==0.6.13\n",
            "  Downloading timm-0.6.13-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.12/dist-packages (0.1.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from timm==0.6.13) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.6.13) (0.24.0+cu126)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from timm==0.6.13) (0.36.0)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia)\n",
            "  Downloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia) (25.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.0)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch-lightning) (4.15.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.13.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.13) (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.6.13) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.6.13) (1.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7->timm==0.6.13) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7->timm==0.6.13) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (2026.1.4)\n",
            "Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.2-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, kornia_rs, torchmetrics, kornia, timm, pytorch-lightning\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.24\n",
            "    Uninstalling timm-1.0.24:\n",
            "      Successfully uninstalled timm-1.0.24\n",
            "Successfully installed kornia-0.8.2 kornia_rs-0.1.10 lightning-utilities-0.15.2 pytorch-lightning-2.6.0 timm-0.6.13 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Path set karo\n",
        "%cd /content/FusionShadowRemoval-main\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "# Direct import try karo (Agar ye fail hua to asli wajah dikhegi)\n",
        "from models.model_convnext import fusion_net\n",
        "print(\"‚úÖ Agar ye print hua, toh model import ho gaya!\")"
      ],
      "metadata": {
        "id": "SIy4_-9D1loC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ccd481-d26c-426a-a23e-cbdd85722a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FusionShadowRemoval-main\n",
            "‚úÖ Agar ye print hua, toh model import ho gaya!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "basic loss function not the one used in fusion net"
      ],
      "metadata": {
        "id": "UTRc7uCS6kL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import shutil\n",
        "\n",
        "# --- 1. SETUP PATHS ---\n",
        "%cd /content/FusionShadowRemoval-main\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/Shadow removal\"\n",
        "LOCAL_BASE = \"/content/dataset\"\n",
        "\n",
        "# --- 2. FORCE COPY DATA (FIXED) ---\n",
        "def force_copy_data():\n",
        "    print(\"üîÑ Checking Data Integrity...\")\n",
        "\n",
        "    # Agar purana folder hai, toh usko uda do taaki fresh copy ho sake\n",
        "    if os.path.exists(LOCAL_BASE):\n",
        "        print(\"üóëÔ∏è Removing old/incomplete local data...\")\n",
        "        shutil.rmtree(LOCAL_BASE)\n",
        "\n",
        "    print(\"‚è≥ Copying data from Drive to Colab Local Disk...\")\n",
        "    os.makedirs(LOCAL_BASE, exist_ok=True)\n",
        "\n",
        "    folders = [\n",
        "        \"ntire24_shrem_train_inp\", \"ntire24_shrem_train_gt\",\n",
        "        \"ntire24_shrem_valid_inp\", \"ntire24_shrem_valid_gt\"\n",
        "    ]\n",
        "\n",
        "    for folder in folders:\n",
        "        src = os.path.join(DRIVE_BASE, folder)\n",
        "        dst = os.path.join(LOCAL_BASE, folder)\n",
        "\n",
        "        if os.path.exists(src):\n",
        "            # Check source file count\n",
        "            num_files = len(os.listdir(src))\n",
        "            print(f\"   üìÇ Copying {folder} ({num_files} images)...\")\n",
        "            shutil.copytree(src, dst)\n",
        "        else:\n",
        "            print(f\"‚ùå Error: Folder '{folder}' Drive pe nahi mila! Check path: {src}\")\n",
        "\n",
        "    print(\"‚úÖ Data Copy Complete! Training should start now.\")\n",
        "\n",
        "# Call the function\n",
        "force_copy_data()\n",
        "\n",
        "# --- 3. IMPORT MODEL ---\n",
        "try:\n",
        "    from models.model_convnext import fusion_net\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Model Import Error: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 4. CONFIGURATION ---\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 1\n",
        "PATCH_SIZE = 256\n",
        "LR = 2e-4\n",
        "\n",
        "class ShadowDataset(Dataset):\n",
        "    def __init__(self, inp_dir, gt_dir, is_train=True):\n",
        "        self.inp_files = sorted(glob.glob(os.path.join(inp_dir, \"*\")))\n",
        "        self.gt_dir = gt_dir\n",
        "        self.is_train = is_train\n",
        "\n",
        "        # Valid pairs check\n",
        "        self.valid_files = [f for f in self.inp_files if os.path.exists(os.path.join(gt_dir, os.path.basename(f)))]\n",
        "\n",
        "        # DEBUG PRINT\n",
        "        if len(self.valid_files) == 0:\n",
        "            print(f\"‚ùå CRITICAL ERROR: No images found in {inp_dir}\")\n",
        "            print(f\"   Checking GT dir: {gt_dir}\")\n",
        "        else:\n",
        "            print(f\"[{'TRAIN' if is_train else 'VALID'}] Loaded {len(self.valid_files)} images.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_path = self.valid_files[idx]\n",
        "        fname = os.path.basename(inp_path)\n",
        "        gt_path = os.path.join(self.gt_dir, fname)\n",
        "\n",
        "        img_inp = cv2.imread(inp_path)\n",
        "        img_gt = cv2.imread(gt_path)\n",
        "\n",
        "        if self.is_train:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            if h > PATCH_SIZE and w > PATCH_SIZE:\n",
        "                x = random.randint(0, w - PATCH_SIZE)\n",
        "                y = random.randint(0, h - PATCH_SIZE)\n",
        "                img_inp = img_inp[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "                img_gt = img_gt[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "            else:\n",
        "                img_inp = cv2.resize(img_inp, (PATCH_SIZE, PATCH_SIZE))\n",
        "                img_gt = cv2.resize(img_gt, (PATCH_SIZE, PATCH_SIZE))\n",
        "        else:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            new_h = ((h // 32) * 32)\n",
        "            new_w = ((w // 32) * 32)\n",
        "            if new_h != h or new_w != w:\n",
        "                img_inp = cv2.resize(img_inp, (new_w, new_h))\n",
        "                img_gt = cv2.resize(img_gt, (new_w, new_h))\n",
        "\n",
        "        inp_t = torch.from_numpy(img_inp[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        gt_t = torch.from_numpy(img_gt[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        return inp_t, gt_t\n",
        "\n",
        "def train():\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"üöÄ Training Started on {device}\")\n",
        "\n",
        "    # Initialize Datasets\n",
        "    train_ds = ShadowDataset(\n",
        "        inp_dir=os.path.join(LOCAL_BASE, \"ntire24_shrem_train_inp\"),\n",
        "        gt_dir=os.path.join(LOCAL_BASE, \"ntire24_shrem_train_gt\"),\n",
        "        is_train=True\n",
        "    )\n",
        "\n",
        "    # Check if dataset is empty before creating loader\n",
        "    if len(train_ds) == 0:\n",
        "        print(\"‚ùå Training Dataset is EMPTY. Stopping.\")\n",
        "        return\n",
        "\n",
        "    val_ds = ShadowDataset(\n",
        "        inp_dir=os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_inp\"),\n",
        "        gt_dir=os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_gt\"),\n",
        "        is_train=False\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = fusion_net().to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    pkl_path = os.path.join(DRIVE_BASE, \"shadowremoval.pkl\")\n",
        "    if os.path.exists(pkl_path):\n",
        "        model.load_state_dict(torch.load(pkl_path, map_location=device), strict=False)\n",
        "        print(\"‚úÖ Pre-trained weights loaded!\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "        for i, (inp, gt) in enumerate(train_loader):\n",
        "            inp, gt = inp.to(device), gt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(inp)\n",
        "            loss = criterion(out, gt)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            if i % 20 == 0:\n",
        "                print(f\"Iter {i}: Loss {loss.item():.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        psnr_list = []\n",
        "        with torch.no_grad():\n",
        "            for inp, gt in val_loader:\n",
        "                inp, gt = inp.to(device), gt.to(device)\n",
        "                out = model(inp)\n",
        "                mse = torch.mean((out - gt) ** 2).item()\n",
        "                psnr = 100 if mse == 0 else 20 * np.log10(1.0 / np.sqrt(mse))\n",
        "                psnr_list.append(psnr)\n",
        "\n",
        "        avg_psnr = np.mean(psnr_list)\n",
        "        print(f\"üî¥ EPOCH {epoch+1} FINISHED. Val PSNR: {avg_psnr:.4f} dB\")\n",
        "\n",
        "        save_path = os.path.join(DRIVE_BASE, f\"model_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"üíæ Saved: {save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "-0SUnfRgu9Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia lpips piq timm==0.6.13"
      ],
      "metadata": {
        "id": "vjI2Jd_OkdRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff17113-452d-40c4-9c47-d71b7e959e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kornia in /usr/local/lib/python3.12/dist-packages (0.8.2)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.12/dist-packages (0.1.4)\n",
            "Collecting piq\n",
            "  Downloading piq-0.8.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: timm==0.6.13 in /usr/local/lib/python3.12/dist-packages (0.6.13)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.12/dist-packages (from timm==0.6.13) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.6.13) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm==0.6.13) (6.0.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from timm==0.6.13) (0.36.0)\n",
            "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from kornia) (0.1.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia) (25.0)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7->timm==0.6.13) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.6.13) (11.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.6.13) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->timm==0.6.13) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7->timm==0.6.13) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.7->timm==0.6.13) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->timm==0.6.13) (2026.1.4)\n",
            "Downloading piq-0.8.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: piq\n",
            "Successfully installed piq-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "correct loss function"
      ],
      "metadata": {
        "id": "bmTgrWMr6ROQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import lpips\n",
        "from piq import MultiScaleSSIMLoss\n",
        "\n",
        "# --- 1. SETUP PATHS ---\n",
        "%cd /content/FusionShadowRemoval-main\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/Shadow removal\"\n",
        "LOCAL_BASE = \"/content/dataset\"\n",
        "\n",
        "# --- IMPORT MODEL ---\n",
        "try:\n",
        "    from models.model_convnext import fusion_net\n",
        "    print(\"‚úÖ Model imported!\")\n",
        "except ImportError:\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 2. ADVANCED LOSSES ---\n",
        "class FocalFrequencyLoss(nn.Module):\n",
        "    def __init__(self, loss_weight=1.0, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "        self.alpha = alpha\n",
        "    def forward(self, pred, target):\n",
        "        return self.loss_weight * torch.mean(torch.abs(torch.fft.rfftn(pred) - torch.fft.rfftn(target)))\n",
        "\n",
        "# --- 3. CONFIGURATION ---\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 4\n",
        "PATCH_SIZE = 256\n",
        "LR = 1e-5\n",
        "\n",
        "# Dataset Class\n",
        "class ShadowDataset(Dataset):\n",
        "    def __init__(self, inp_dir, gt_dir, is_train=True):\n",
        "        self.inp_files = sorted(glob.glob(os.path.join(inp_dir, \"*\")))\n",
        "        self.gt_dir = gt_dir\n",
        "        self.is_train = is_train\n",
        "        self.valid_files = [f for f in self.inp_files if os.path.exists(os.path.join(gt_dir, os.path.basename(f)))]\n",
        "\n",
        "    def __len__(self): return len(self.valid_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_path = self.valid_files[idx]\n",
        "        fname = os.path.basename(inp_path)\n",
        "        gt_path = os.path.join(self.gt_dir, fname)\n",
        "        img_inp = cv2.imread(inp_path)\n",
        "        img_gt = cv2.imread(gt_path)\n",
        "\n",
        "        if self.is_train:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            if h > PATCH_SIZE and w > PATCH_SIZE:\n",
        "                x = random.randint(0, w - PATCH_SIZE); y = random.randint(0, h - PATCH_SIZE)\n",
        "                img_inp = img_inp[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "                img_gt = img_gt[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "            else:\n",
        "                img_inp = cv2.resize(img_inp, (PATCH_SIZE, PATCH_SIZE))\n",
        "                img_gt = cv2.resize(img_gt, (PATCH_SIZE, PATCH_SIZE))\n",
        "        else:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            new_h, new_w = ((h // 32) * 32), ((w // 32) * 32)\n",
        "            img_inp = cv2.resize(img_inp, (new_w, new_h))\n",
        "            img_gt = cv2.resize(img_gt, (new_w, new_h))\n",
        "\n",
        "        inp_t = torch.from_numpy(img_inp[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        gt_t = torch.from_numpy(img_gt[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        return inp_t, gt_t\n",
        "\n",
        "def train():\n",
        "    device = torch.device('cuda:0')\n",
        "    print(f\"üöÄ Training with Advanced Losses on {device}\")\n",
        "\n",
        "    # Check Local Data\n",
        "    if not os.path.exists(LOCAL_BASE):\n",
        "        print(\"‚ùå Local Data not found! Please run the 'Copy Data' script first.\")\n",
        "        return\n",
        "\n",
        "    train_ds = ShadowDataset(os.path.join(LOCAL_BASE, \"ntire24_shrem_train_inp\"), os.path.join(LOCAL_BASE, \"ntire24_shrem_train_gt\"), True)\n",
        "    val_ds = ShadowDataset(os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_inp\"), os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_gt\"), False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = fusion_net().to(device)\n",
        "\n",
        "    # Initialize Losses\n",
        "    l1_crit = nn.L1Loss().to(device)\n",
        "    lpips_crit = lpips.LPIPS(net='vgg').to(device)\n",
        "    fft_crit = FocalFrequencyLoss(loss_weight=0.1, alpha=1.0).to(device)\n",
        "    msssim_crit = MultiScaleSSIMLoss(data_range=1.0).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=5e-6)\n",
        "\n",
        "    # Load Weights\n",
        "    pkl_path = os.path.join(DRIVE_BASE, \"shadowremoval.pkl\")\n",
        "    if os.path.exists(pkl_path):\n",
        "        model.load_state_dict(torch.load(pkl_path, map_location=device), strict=False)\n",
        "        print(\"‚úÖ Pre-trained weights loaded!\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "        for i, (inp, gt) in enumerate(train_loader):\n",
        "            inp, gt = inp.to(device), gt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward\n",
        "            out = model(inp)\n",
        "\n",
        "            # --- THE FIX: CLAMPING ---\n",
        "            # SSIM aur LPIPS ke liye output 0-1 ke beech hona zaroori hai\n",
        "            out_clamped = torch.clamp(out, 0, 1)\n",
        "\n",
        "            # 1. L1 Loss (Raw output pe, taaki model negative dena band kare)\n",
        "            l1 = l1_crit(out, gt)\n",
        "\n",
        "            # 2. Strict Losses (Clamped output pe)\n",
        "            perceptual = lpips_crit(out_clamped, gt).mean()\n",
        "            fft = fft_crit(out_clamped, gt)\n",
        "            ms_ssim = 1 - msssim_crit(out_clamped, gt)\n",
        "\n",
        "            # Weighted Sum\n",
        "            loss = l1 + (0.05 * perceptual) + (0.1 * fft) + (0.2 * ms_ssim)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if i % 20 == 0:\n",
        "                print(f\"Iter {i}: Loss {loss.item():.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        psnr_list = []\n",
        "        with torch.no_grad():\n",
        "            for inp, gt in val_loader:\n",
        "                inp, gt = inp.to(device), gt.to(device)\n",
        "                out = model(inp)\n",
        "                # Validation mein bhi clamp kar dete hain safe side ke liye\n",
        "                out = torch.clamp(out, 0, 1)\n",
        "\n",
        "                mse = torch.mean((out - gt) ** 2).item()\n",
        "                psnr = 100 if mse == 0 else 20 * np.log10(1.0 / np.sqrt(mse))\n",
        "                psnr_list.append(psnr)\n",
        "\n",
        "        avg_psnr = np.mean(psnr_list)\n",
        "        print(f\"üî¥ EPOCH {epoch+1} DONE. Val PSNR: {avg_psnr:.4f} dB\")\n",
        "\n",
        "        save_path = os.path.join(DRIVE_BASE, f\"model_pro_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"üíæ Saved: {save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "4sFj7lZHkbcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "minor changes in loss function"
      ],
      "metadata": {
        "id": "sFfBv52u6wZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade cython numpy setuptools wheel"
      ],
      "metadata": {
        "id": "VFkL4ca7Fzbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import shutil\n",
        "import lpips\n",
        "from piq import MultiScaleSSIMLoss\n",
        "\n",
        "# --- 1. SETUP PATHS ---\n",
        "%cd /content/FusionShadowRemoval-main\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/Shadow removal\"\n",
        "LOCAL_BASE = \"/content/dataset\"\n",
        "\n",
        "# --- 2. FORCE DATA REPAIR (THE FIX) ---\n",
        "def force_fix_data():\n",
        "    print(\"üîÑ Checking Dataset Health...\")\n",
        "\n",
        "    # Check if data exists AND is not empty\n",
        "    inp_path = os.path.join(LOCAL_BASE, \"ntire24_shrem_train_inp\")\n",
        "\n",
        "    # Agar folder nahi hai YA folder khaali hai -> Re-copy karo\n",
        "    if not os.path.exists(inp_path) or len(os.listdir(inp_path)) == 0:\n",
        "        print(\"‚ö†Ô∏è Data missing or empty! Re-copying from Drive...\")\n",
        "\n",
        "        # Purana kachra saaf karo\n",
        "        if os.path.exists(LOCAL_BASE):\n",
        "            shutil.rmtree(LOCAL_BASE)\n",
        "\n",
        "        # Fresh Copy\n",
        "        os.makedirs(LOCAL_BASE, exist_ok=True)\n",
        "        for folder in [\"ntire24_shrem_train_inp\", \"ntire24_shrem_train_gt\", \"ntire24_shrem_valid_inp\", \"ntire24_shrem_valid_gt\"]:\n",
        "            src = os.path.join(DRIVE_BASE, folder)\n",
        "            dst = os.path.join(LOCAL_BASE, folder)\n",
        "            if os.path.exists(src):\n",
        "                print(f\"   üìÇ Copying {folder}...\")\n",
        "                shutil.copytree(src, dst)\n",
        "            else:\n",
        "                print(f\"‚ùå Error: {folder} Drive pe nahi mila!\")\n",
        "        print(\"‚úÖ Data Restoration Complete!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Data is present and ready.\")\n",
        "\n",
        "force_fix_data()\n",
        "\n",
        "# --- IMPORT MODEL ---\n",
        "try:\n",
        "    from models.model_convnext import fusion_net\n",
        "    print(\"‚úÖ Model imported!\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå Error: Model import fail hua.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 3. ADVANCED LOSSES ---\n",
        "class FocalFrequencyLoss(nn.Module):\n",
        "    def __init__(self, loss_weight=1.0, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "        self.alpha = alpha\n",
        "    def forward(self, pred, target):\n",
        "        return self.loss_weight * torch.mean(torch.abs(torch.fft.rfftn(pred) - torch.fft.rfftn(target)))\n",
        "\n",
        "# --- 4. CONFIGURATION ---\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 4\n",
        "PATCH_SIZE = 256\n",
        "LR = 5e-6\n",
        "\n",
        "# Dataset Class\n",
        "class ShadowDataset(Dataset):\n",
        "    def __init__(self, inp_dir, gt_dir, is_train=True):\n",
        "        self.inp_files = sorted(glob.glob(os.path.join(inp_dir, \"*\")))\n",
        "        self.gt_dir = gt_dir\n",
        "        self.is_train = is_train\n",
        "        self.valid_files = [f for f in self.inp_files if os.path.exists(os.path.join(gt_dir, os.path.basename(f)))]\n",
        "\n",
        "    def __len__(self): return len(self.valid_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_path = self.valid_files[idx]\n",
        "        fname = os.path.basename(inp_path)\n",
        "        gt_path = os.path.join(self.gt_dir, fname)\n",
        "        img_inp = cv2.imread(inp_path)\n",
        "        img_gt = cv2.imread(gt_path)\n",
        "\n",
        "        if self.is_train:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            if h > PATCH_SIZE and w > PATCH_SIZE:\n",
        "                x = random.randint(0, w - PATCH_SIZE); y = random.randint(0, h - PATCH_SIZE)\n",
        "                img_inp = img_inp[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "                img_gt = img_gt[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "            else:\n",
        "                img_inp = cv2.resize(img_inp, (PATCH_SIZE, PATCH_SIZE))\n",
        "                img_gt = cv2.resize(img_gt, (PATCH_SIZE, PATCH_SIZE))\n",
        "        else:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            new_h, new_w = ((h // 32) * 32), ((w // 32) * 32)\n",
        "            img_inp = cv2.resize(img_inp, (new_w, new_h))\n",
        "            img_gt = cv2.resize(img_gt, (new_w, new_h))\n",
        "\n",
        "        inp_t = torch.from_numpy(img_inp[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        gt_t = torch.from_numpy(img_gt[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        return inp_t, gt_t\n",
        "\n",
        "def train():\n",
        "    device = torch.device('cuda:0')\n",
        "    print(f\"üöÄ Fine-Tuning Started on {device} (Target: 27+ dB)\")\n",
        "\n",
        "    train_ds = ShadowDataset(os.path.join(LOCAL_BASE, \"ntire24_shrem_train_inp\"), os.path.join(LOCAL_BASE, \"ntire24_shrem_train_gt\"), True)\n",
        "\n",
        "    # Final Safety Check\n",
        "    if len(train_ds) == 0:\n",
        "        print(\"‚ùå CRITICAL: Dataset is still empty! Check Drive paths.\")\n",
        "        return\n",
        "\n",
        "    val_ds = ShadowDataset(os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_inp\"), os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_gt\"), False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = fusion_net().to(device)\n",
        "\n",
        "    # Losses\n",
        "    l1_crit = nn.L1Loss().to(device)\n",
        "    lpips_crit = lpips.LPIPS(net='vgg').to(device)\n",
        "    fft_crit = FocalFrequencyLoss(loss_weight=0.1, alpha=1.0).to(device)\n",
        "    msssim_crit = MultiScaleSSIMLoss(data_range=1.0).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=1e-7)\n",
        "\n",
        "    # Resume Logic\n",
        "    finetune_weight = os.path.join(DRIVE_BASE, \"model_pro_epoch_5.pth\")\n",
        "    if os.path.exists(finetune_weight):\n",
        "        model.load_state_dict(torch.load(finetune_weight, map_location=device), strict=False)\n",
        "        print(f\"‚úÖ Loaded Best Weight: {finetune_weight}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Warning: Previous weight not found. Starting fresh/base.\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\n--- Fine-Tune Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "        for i, (inp, gt) in enumerate(train_loader):\n",
        "            inp, gt = inp.to(device), gt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(inp)\n",
        "            out_clamped = torch.clamp(out, 0, 1)\n",
        "\n",
        "            loss = l1_crit(out, gt) + (0.05 * lpips_crit(out_clamped, gt).mean()) + fft_crit(out_clamped, gt) + (0.2 * (1 - msssim_crit(out_clamped, gt)))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Iter {i}: Loss {loss.item():.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        psnr_list = []\n",
        "        with torch.no_grad():\n",
        "            for inp, gt in val_loader:\n",
        "                inp, gt = inp.to(device), gt.to(device)\n",
        "                out = torch.clamp(model(inp), 0, 1)\n",
        "                mse = torch.mean((out - gt) ** 2).item()\n",
        "                psnr = 100 if mse == 0 else 20 * np.log10(1.0 / np.sqrt(mse))\n",
        "                psnr_list.append(psnr)\n",
        "\n",
        "        avg_psnr = np.mean(psnr_list)\n",
        "        print(f\"üî¥ EPOCH {epoch+1} DONE. Val PSNR: {avg_psnr:.4f} dB\")\n",
        "\n",
        "        torch.save(model.state_dict(), os.path.join(DRIVE_BASE, f\"model_finetune_epoch_{epoch+1}.pth\"))\n",
        "        print(f\"üíæ Saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "-WhFstKvGeD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "some more changes in parameters trying to get better\n"
      ],
      "metadata": {
        "id": "2wnBoIz9vbEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install piq kornia lpips timm==0.6.13"
      ],
      "metadata": {
        "id": "-GrYnSHfwOYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import shutil\n",
        "import lpips\n",
        "from piq import MultiScaleSSIMLoss\n",
        "\n",
        "# --- 1. SETUP PATHS ---\n",
        "%cd /content/FusionShadowRemoval-main\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "DRIVE_BASE = \"/content/drive/MyDrive/Shadow removal\"\n",
        "LOCAL_BASE = \"/content/dataset\"\n",
        "\n",
        "# --- 2. FORCE DATA REPAIR (THE FIX) ---\n",
        "def force_fix_data():\n",
        "    print(\"üîÑ Checking Dataset Health...\")\n",
        "\n",
        "    # Check if data exists AND is not empty\n",
        "    inp_path = os.path.join(LOCAL_BASE, \"ntire24_shrem_train_inp\")\n",
        "\n",
        "    # Agar folder nahi hai YA folder khaali hai -> Re-copy karo\n",
        "    if not os.path.exists(inp_path) or len(os.listdir(inp_path)) == 0:\n",
        "        print(\"‚ö†Ô∏è Data missing or empty! Re-copying from Drive...\")\n",
        "\n",
        "        # Purana kachra saaf karo\n",
        "        if os.path.exists(LOCAL_BASE):\n",
        "            shutil.rmtree(LOCAL_BASE)\n",
        "\n",
        "        # Fresh Copy\n",
        "        os.makedirs(LOCAL_BASE, exist_ok=True)\n",
        "        for folder in [\"ntire24_shrem_train_inp\", \"ntire24_shrem_train_gt\", \"ntire24_shrem_valid_inp\", \"ntire24_shrem_valid_gt\"]:\n",
        "            src = os.path.join(DRIVE_BASE, folder)\n",
        "            dst = os.path.join(LOCAL_BASE, folder)\n",
        "            if os.path.exists(src):\n",
        "                print(f\"   üìÇ Copying {folder}...\")\n",
        "                shutil.copytree(src, dst)\n",
        "            else:\n",
        "                print(f\"‚ùå Error: {folder} Drive pe nahi mila!\")\n",
        "        print(\"‚úÖ Data Restoration Complete!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Data is present and ready.\")\n",
        "\n",
        "force_fix_data()\n",
        "\n",
        "\n",
        "# --- 2. IMPORT MODEL ---\n",
        "try:\n",
        "    from models.model_convnext import fusion_net\n",
        "    print(\"‚úÖ Model imported!\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå Error: Model import fail hua.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 3. ADVANCED LOSSES ---\n",
        "class FocalFrequencyLoss(nn.Module):\n",
        "    def __init__(self, loss_weight=1.0, alpha=1.0):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "        self.alpha = alpha\n",
        "    def forward(self, pred, target):\n",
        "        return self.loss_weight * torch.mean(torch.abs(torch.fft.rfftn(pred) - torch.fft.rfftn(target)))\n",
        "\n",
        "# --- 4. CONFIGURATION (SAFE & SLOW) ---\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 1\n",
        "PATCH_SIZE = 256\n",
        "LR = 2e-6             # <-- VERY LOW LR (Safety Lock)\n",
        "\n",
        "# --- 5. DATASET (NO AUGMENTATION - SAFE MODE) ---\n",
        "class ShadowDataset(Dataset):\n",
        "    def __init__(self, inp_dir, gt_dir, is_train=True):\n",
        "        self.inp_files = sorted(glob.glob(os.path.join(inp_dir, \"*\")))\n",
        "        self.gt_dir = gt_dir\n",
        "        self.is_train = is_train\n",
        "        self.valid_files = [f for f in self.inp_files if os.path.exists(os.path.join(gt_dir, os.path.basename(f)))]\n",
        "\n",
        "    def __len__(self): return len(self.valid_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inp_path = self.valid_files[idx]\n",
        "        fname = os.path.basename(inp_path)\n",
        "        gt_path = os.path.join(self.gt_dir, fname)\n",
        "        img_inp = cv2.imread(inp_path)\n",
        "        img_gt = cv2.imread(gt_path)\n",
        "\n",
        "        if self.is_train:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            if h > PATCH_SIZE and w > PATCH_SIZE:\n",
        "                x = random.randint(0, w - PATCH_SIZE); y = random.randint(0, h - PATCH_SIZE)\n",
        "                img_inp = img_inp[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "                img_gt = img_gt[y:y+PATCH_SIZE, x:x+PATCH_SIZE]\n",
        "            else:\n",
        "                img_inp = cv2.resize(img_inp, (PATCH_SIZE, PATCH_SIZE))\n",
        "                img_gt = cv2.resize(img_gt, (PATCH_SIZE, PATCH_SIZE))\n",
        "        else:\n",
        "            h, w = img_inp.shape[:2]\n",
        "            new_h, new_w = ((h // 32) * 32), ((w // 32) * 32)\n",
        "            img_inp = cv2.resize(img_inp, (new_w, new_h))\n",
        "            img_gt = cv2.resize(img_gt, (new_w, new_h))\n",
        "\n",
        "        inp_t = torch.from_numpy(img_inp[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        gt_t = torch.from_numpy(img_gt[:, :, ::-1].copy()).permute(2, 0, 1).float() / 255.0\n",
        "        return inp_t, gt_t\n",
        "\n",
        "def train():\n",
        "    device = torch.device('cuda:0')\n",
        "    print(f\"üöÄ RELIABLE Fine-Tuning Started on {device}\")\n",
        "\n",
        "    if not os.path.exists(LOCAL_BASE):\n",
        "        print(\"‚ùå Local Data missing. Run data copy first.\")\n",
        "        return\n",
        "\n",
        "    train_ds = ShadowDataset(os.path.join(LOCAL_BASE, \"ntire24_shrem_train_inp\"), os.path.join(LOCAL_BASE, \"ntire24_shrem_train_gt\"), True)\n",
        "    val_ds = ShadowDataset(os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_inp\"), os.path.join(LOCAL_BASE, \"ntire24_shrem_valid_gt\"), False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = fusion_net().to(device)\n",
        "\n",
        "    # Standard Losses (Jo pehle kaam kiye the)\n",
        "    l1_crit = nn.L1Loss().to(device)\n",
        "    lpips_crit = lpips.LPIPS(net='vgg').to(device)\n",
        "    fft_crit = FocalFrequencyLoss(loss_weight=0.1, alpha=1.0).to(device)\n",
        "    msssim_crit = MultiScaleSSIMLoss(data_range=1.0).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-8)\n",
        "\n",
        "    # Scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS, eta_min=1e-7)\n",
        "\n",
        "    # --- CRITICAL: LOAD THE GOOD WEIGHTS ---\n",
        "    # Hum specific file load karenge jo humein pata hai achhi thi\n",
        "    good_weight = os.path.join(DRIVE_BASE, \"model_pro_epoch_5.pth\")\n",
        "\n",
        "    if os.path.exists(good_weight):\n",
        "        print(f\"‚úÖ RECOVERING: Loading best model (26.1 dB): {good_weight}\")\n",
        "        model.load_state_dict(torch.load(good_weight, map_location=device), strict=False)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Warning: Good weight not found! Checking others...\")\n",
        "        # Fallback logic to avoid loading the bad \"boost\" model\n",
        "        all_weights = sorted(glob.glob(os.path.join(DRIVE_BASE, \"*.pth\")), key=os.path.getmtime)\n",
        "        valid_weights = [w for w in all_weights if \"boost\" not in w] # Exclude bad booster weights\n",
        "        if valid_weights:\n",
        "            print(f\"‚úÖ Loading: {valid_weights[-1]}\")\n",
        "            model.load_state_dict(torch.load(valid_weights[-1], map_location=device), strict=False)\n",
        "\n",
        "    best_psnr = 26.0 # Baseline\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        print(f\"\\n--- Reliable Epoch {epoch+1}/{EPOCHS} ---\")\n",
        "        for i, (inp, gt) in enumerate(train_loader):\n",
        "            inp, gt = inp.to(device), gt.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(inp)\n",
        "            out_clamped = torch.clamp(out, 0, 1)\n",
        "\n",
        "            # Original Formula (No Greedy, No Risks)\n",
        "            l1 = l1_crit(out, gt)\n",
        "            perceptual = lpips_crit(out_clamped, gt).mean()\n",
        "            fft = fft_crit(out_clamped, gt)\n",
        "            ms_ssim = 1 - msssim_crit(out_clamped, gt)\n",
        "\n",
        "            # Standard Weights\n",
        "            loss = l1 + (0.05 * perceptual) + (0.1 * fft) + (0.2 * ms_ssim)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            if i % 50 == 0:\n",
        "                print(f\"Iter {i}: Loss {loss.item():.4f}\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        psnr_list = []\n",
        "        with torch.no_grad():\n",
        "            for inp, gt in val_loader:\n",
        "                inp, gt = inp.to(device), gt.to(device)\n",
        "                out = torch.clamp(model(inp), 0, 1)\n",
        "                mse = torch.mean((out - gt) ** 2).item()\n",
        "                psnr = 100 if mse == 0 else 20 * np.log10(1.0 / np.sqrt(mse))\n",
        "                psnr_list.append(psnr)\n",
        "\n",
        "        avg_psnr = np.mean(psnr_list)\n",
        "        print(f\"üî¥ EPOCH {epoch+1} DONE. Val PSNR: {avg_psnr:.4f} dB\")\n",
        "\n",
        "        # Safe Name\n",
        "        save_path = os.path.join(DRIVE_BASE, f\"model_refined_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"üíæ Saved: {save_path}\")\n",
        "\n",
        "        if avg_psnr > best_psnr:\n",
        "            best_psnr = avg_psnr\n",
        "            print(\"‚ú® Improvement detected!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "id": "qLz8k386veJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de4e01d-f904-40b3-8732-d79021456cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FusionShadowRemoval-main\n",
            "üîÑ Checking Dataset Health...\n",
            "‚ö†Ô∏è Data missing or empty! Re-copying from Drive...\n",
            "   üìÇ Copying ntire24_shrem_train_inp...\n",
            "   üìÇ Copying ntire24_shrem_train_gt...\n",
            "   üìÇ Copying ntire24_shrem_valid_inp...\n",
            "   üìÇ Copying ntire24_shrem_valid_gt...\n",
            "‚úÖ Data Restoration Complete!\n",
            "‚úÖ Model imported!\n",
            "üöÄ RELIABLE Fine-Tuning Started on cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:07<00:00, 78.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/vgg.pth\n",
            "‚úÖ RECOVERING: Loading best model (26.1 dB): /content/drive/MyDrive/Shadow removal/model_pro_epoch_5.pth\n",
            "\n",
            "--- Reliable Epoch 1/20 ---\n",
            "Iter 0: Loss 0.2190\n",
            "Iter 50: Loss 0.2834\n",
            "Iter 100: Loss 0.2658\n",
            "Iter 150: Loss 0.2483\n",
            "Iter 200: Loss 0.2226\n",
            "Iter 250: Loss 0.2834\n",
            "Iter 300: Loss 0.3640\n",
            "Iter 350: Loss 0.2188\n",
            "Iter 400: Loss 0.2929\n",
            "Iter 450: Loss 0.2989\n",
            "Iter 500: Loss 0.2341\n",
            "Iter 550: Loss 0.2679\n",
            "Iter 600: Loss 0.2691\n",
            "Iter 650: Loss 0.2308\n",
            "Iter 700: Loss 0.3192\n",
            "Iter 750: Loss 0.2554\n",
            "Iter 800: Loss 0.3314\n",
            "Iter 850: Loss 0.2451\n",
            "Iter 900: Loss 0.2609\n",
            "Iter 950: Loss 0.2320\n",
            "üî¥ EPOCH 1 DONE. Val PSNR: 26.1406 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_1.pth\n",
            "‚ú® Improvement detected!\n",
            "\n",
            "--- Reliable Epoch 2/20 ---\n",
            "Iter 0: Loss 0.2962\n",
            "Iter 50: Loss 0.2909\n",
            "Iter 100: Loss 0.3560\n",
            "Iter 150: Loss 0.2162\n",
            "Iter 200: Loss 0.2922\n",
            "Iter 250: Loss 0.2348\n",
            "Iter 300: Loss 0.2211\n",
            "Iter 350: Loss 0.3314\n",
            "Iter 400: Loss 0.2602\n",
            "Iter 450: Loss 0.2229\n",
            "Iter 500: Loss 0.3089\n",
            "Iter 550: Loss 0.2991\n",
            "Iter 600: Loss 0.3239\n",
            "Iter 650: Loss 0.2468\n",
            "Iter 700: Loss 0.2714\n",
            "Iter 750: Loss 0.2974\n",
            "Iter 800: Loss 0.3122\n",
            "Iter 850: Loss 0.2889\n",
            "Iter 900: Loss 0.3600\n",
            "Iter 950: Loss 0.2463\n",
            "üî¥ EPOCH 2 DONE. Val PSNR: 26.1502 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_2.pth\n",
            "‚ú® Improvement detected!\n",
            "\n",
            "--- Reliable Epoch 3/20 ---\n",
            "Iter 0: Loss 0.2511\n",
            "Iter 50: Loss 0.2291\n",
            "Iter 100: Loss 0.2574\n",
            "Iter 150: Loss 0.2140\n",
            "Iter 200: Loss 0.2536\n",
            "Iter 250: Loss 0.2652\n",
            "Iter 300: Loss 0.2226\n",
            "Iter 350: Loss 0.2468\n",
            "Iter 400: Loss 0.2711\n",
            "Iter 450: Loss 0.2673\n",
            "Iter 500: Loss 0.3000\n",
            "Iter 550: Loss 0.2780\n",
            "Iter 600: Loss 0.2905\n",
            "Iter 650: Loss 0.2699\n",
            "Iter 700: Loss 0.2940\n",
            "Iter 750: Loss 0.2974\n",
            "Iter 800: Loss 0.2741\n",
            "Iter 850: Loss 0.3278\n",
            "Iter 900: Loss 0.2739\n",
            "Iter 950: Loss 0.3184\n",
            "üî¥ EPOCH 3 DONE. Val PSNR: 26.0937 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_3.pth\n",
            "\n",
            "--- Reliable Epoch 4/20 ---\n",
            "Iter 0: Loss 0.3188\n",
            "Iter 50: Loss 0.2570\n",
            "Iter 100: Loss 0.2224\n",
            "Iter 150: Loss 0.2891\n",
            "Iter 200: Loss 0.2894\n",
            "Iter 250: Loss 0.2187\n",
            "Iter 300: Loss 0.3023\n",
            "Iter 350: Loss 0.3397\n",
            "Iter 400: Loss 0.3174\n",
            "Iter 450: Loss 0.3089\n",
            "Iter 500: Loss 0.2271\n",
            "Iter 550: Loss 0.2738\n",
            "Iter 600: Loss 0.3272\n",
            "Iter 650: Loss 0.2585\n",
            "Iter 700: Loss 0.2456\n",
            "Iter 750: Loss 0.2696\n",
            "Iter 800: Loss 0.2835\n",
            "Iter 850: Loss 0.2490\n",
            "Iter 900: Loss 0.2212\n",
            "Iter 950: Loss 0.3278\n",
            "üî¥ EPOCH 4 DONE. Val PSNR: 26.1246 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_4.pth\n",
            "\n",
            "--- Reliable Epoch 5/20 ---\n",
            "Iter 0: Loss 0.2595\n",
            "Iter 50: Loss 0.3805\n",
            "Iter 100: Loss 0.3111\n",
            "Iter 150: Loss 0.2704\n",
            "Iter 200: Loss 0.2322\n",
            "Iter 250: Loss 0.2273\n",
            "Iter 300: Loss 0.3543\n",
            "Iter 350: Loss 0.2999\n",
            "Iter 400: Loss 0.3070\n",
            "Iter 450: Loss 0.2251\n",
            "Iter 500: Loss 0.2501\n",
            "Iter 550: Loss 0.2958\n",
            "Iter 600: Loss 0.2937\n",
            "Iter 650: Loss 0.3146\n",
            "Iter 700: Loss 0.2627\n",
            "Iter 750: Loss 0.3307\n",
            "Iter 800: Loss 0.2611\n",
            "Iter 850: Loss 0.2436\n",
            "Iter 900: Loss 0.2664\n",
            "Iter 950: Loss 0.2437\n",
            "üî¥ EPOCH 5 DONE. Val PSNR: 26.1412 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_5.pth\n",
            "\n",
            "--- Reliable Epoch 6/20 ---\n",
            "Iter 0: Loss 0.2370\n",
            "Iter 50: Loss 0.2846\n",
            "Iter 100: Loss 0.3189\n",
            "Iter 150: Loss 0.2954\n",
            "Iter 200: Loss 0.2727\n",
            "Iter 250: Loss 0.2985\n",
            "Iter 300: Loss 0.2464\n",
            "Iter 350: Loss 0.2599\n",
            "Iter 400: Loss 0.2319\n",
            "Iter 450: Loss 0.2273\n",
            "Iter 500: Loss 0.2259\n",
            "Iter 550: Loss 0.2273\n",
            "Iter 600: Loss 0.2224\n",
            "Iter 650: Loss 0.2355\n",
            "Iter 700: Loss 0.3070\n",
            "Iter 750: Loss 0.3359\n",
            "Iter 800: Loss 0.3445\n",
            "Iter 850: Loss 0.2772\n",
            "Iter 900: Loss 0.2961\n",
            "Iter 950: Loss 0.2654\n",
            "üî¥ EPOCH 6 DONE. Val PSNR: 26.0774 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_6.pth\n",
            "\n",
            "--- Reliable Epoch 7/20 ---\n",
            "Iter 0: Loss 0.2138\n",
            "Iter 50: Loss 0.3069\n",
            "Iter 100: Loss 0.2877\n",
            "Iter 150: Loss 0.2971\n",
            "Iter 200: Loss 0.2579\n",
            "Iter 250: Loss 0.2117\n",
            "Iter 300: Loss 0.2361\n",
            "Iter 350: Loss 0.2544\n",
            "Iter 400: Loss 0.2246\n",
            "Iter 450: Loss 0.4164\n",
            "Iter 500: Loss 0.2510\n",
            "Iter 550: Loss 0.2589\n",
            "Iter 600: Loss 0.2706\n",
            "Iter 650: Loss 0.2452\n",
            "Iter 700: Loss 0.3259\n",
            "Iter 750: Loss 0.3276\n",
            "Iter 800: Loss 0.2557\n",
            "Iter 850: Loss 0.3054\n",
            "Iter 900: Loss 0.2483\n",
            "Iter 950: Loss 0.2771\n",
            "üî¥ EPOCH 7 DONE. Val PSNR: 26.0937 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_7.pth\n",
            "\n",
            "--- Reliable Epoch 8/20 ---\n",
            "Iter 0: Loss 0.2177\n",
            "Iter 50: Loss 0.3022\n",
            "Iter 100: Loss 0.3774\n",
            "Iter 150: Loss 0.2339\n",
            "Iter 200: Loss 0.2649\n",
            "Iter 250: Loss 0.3314\n",
            "Iter 300: Loss 0.2270\n",
            "Iter 350: Loss 0.2565\n",
            "Iter 400: Loss 0.2671\n",
            "Iter 450: Loss 0.3773\n",
            "Iter 500: Loss 0.2562\n",
            "Iter 550: Loss 0.2552\n",
            "Iter 600: Loss 0.3871\n",
            "Iter 650: Loss 0.2472\n",
            "Iter 700: Loss 0.2275\n",
            "Iter 750: Loss 0.2979\n",
            "Iter 800: Loss 0.2992\n",
            "Iter 850: Loss 0.2444\n",
            "Iter 900: Loss 0.3331\n",
            "Iter 950: Loss 0.3182\n",
            "üî¥ EPOCH 8 DONE. Val PSNR: 26.0738 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_8.pth\n",
            "\n",
            "--- Reliable Epoch 9/20 ---\n",
            "Iter 0: Loss 0.3149\n",
            "Iter 50: Loss 0.2488\n",
            "Iter 100: Loss 0.2558\n",
            "Iter 150: Loss 0.2993\n",
            "Iter 200: Loss 0.3015\n",
            "Iter 250: Loss 0.2621\n",
            "Iter 300: Loss 0.2423\n",
            "Iter 350: Loss 0.2687\n",
            "Iter 400: Loss 0.3572\n",
            "Iter 450: Loss 0.3152\n",
            "Iter 500: Loss 0.2658\n",
            "Iter 550: Loss 0.2655\n",
            "Iter 600: Loss 0.2157\n",
            "Iter 650: Loss 0.3391\n",
            "Iter 700: Loss 0.2741\n",
            "Iter 750: Loss 0.2424\n",
            "Iter 800: Loss 0.2580\n",
            "Iter 850: Loss 0.2693\n",
            "Iter 900: Loss 0.4188\n",
            "Iter 950: Loss 0.2932\n",
            "üî¥ EPOCH 9 DONE. Val PSNR: 26.1021 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_9.pth\n",
            "\n",
            "--- Reliable Epoch 10/20 ---\n",
            "Iter 0: Loss 0.3926\n",
            "Iter 50: Loss 0.3380\n",
            "Iter 100: Loss 0.2633\n",
            "Iter 150: Loss 0.2862\n",
            "Iter 200: Loss 0.2825\n",
            "Iter 250: Loss 0.3123\n",
            "Iter 300: Loss 0.2897\n",
            "Iter 350: Loss 0.2680\n",
            "Iter 400: Loss 0.2798\n",
            "Iter 450: Loss 0.2338\n",
            "Iter 500: Loss 0.2575\n",
            "Iter 550: Loss 0.2226\n",
            "Iter 600: Loss 0.3392\n",
            "Iter 650: Loss 0.2478\n",
            "Iter 700: Loss 0.3488\n",
            "Iter 750: Loss 0.3175\n",
            "Iter 800: Loss 0.2551\n",
            "Iter 850: Loss 0.3253\n",
            "Iter 900: Loss 0.2420\n",
            "Iter 950: Loss 0.2312\n",
            "üî¥ EPOCH 10 DONE. Val PSNR: 26.1019 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_10.pth\n",
            "\n",
            "--- Reliable Epoch 11/20 ---\n",
            "Iter 0: Loss 0.3337\n",
            "Iter 50: Loss 0.2395\n",
            "Iter 100: Loss 0.3322\n",
            "Iter 150: Loss 0.2788\n",
            "Iter 200: Loss 0.3429\n",
            "Iter 250: Loss 0.2658\n",
            "Iter 300: Loss 0.2785\n",
            "Iter 350: Loss 0.4133\n",
            "Iter 400: Loss 0.3446\n",
            "Iter 450: Loss 0.2695\n",
            "Iter 500: Loss 0.2673\n",
            "Iter 550: Loss 0.2237\n",
            "Iter 600: Loss 0.2531\n",
            "Iter 650: Loss 0.2134\n",
            "Iter 700: Loss 0.3517\n",
            "Iter 750: Loss 0.2435\n",
            "Iter 800: Loss 0.2717\n",
            "Iter 850: Loss 0.3708\n",
            "Iter 900: Loss 0.2499\n",
            "Iter 950: Loss 0.2427\n",
            "üî¥ EPOCH 11 DONE. Val PSNR: 26.0905 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_11.pth\n",
            "\n",
            "--- Reliable Epoch 12/20 ---\n",
            "Iter 0: Loss 0.2651\n",
            "Iter 50: Loss 0.2505\n",
            "Iter 100: Loss 0.3928\n",
            "Iter 150: Loss 0.2603\n",
            "Iter 200: Loss 0.3983\n",
            "Iter 250: Loss 0.3035\n",
            "Iter 300: Loss 0.3067\n",
            "Iter 350: Loss 0.2285\n",
            "Iter 400: Loss 0.2229\n",
            "Iter 450: Loss 0.2244\n",
            "Iter 500: Loss 0.2583\n",
            "Iter 550: Loss 0.2952\n",
            "Iter 600: Loss 0.3235\n",
            "Iter 650: Loss 0.3231\n",
            "Iter 700: Loss 0.2700\n",
            "Iter 750: Loss 0.2332\n",
            "Iter 800: Loss 0.3195\n",
            "Iter 850: Loss 0.2586\n",
            "Iter 900: Loss 0.2932\n",
            "Iter 950: Loss 0.3018\n",
            "üî¥ EPOCH 12 DONE. Val PSNR: 26.0956 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_12.pth\n",
            "\n",
            "--- Reliable Epoch 13/20 ---\n",
            "Iter 0: Loss 0.2462\n",
            "Iter 50: Loss 0.3553\n",
            "Iter 100: Loss 0.3719\n",
            "Iter 150: Loss 0.2262\n",
            "Iter 200: Loss 0.3122\n",
            "Iter 250: Loss 0.2561\n",
            "Iter 300: Loss 0.3776\n",
            "Iter 350: Loss 0.2438\n",
            "Iter 400: Loss 0.2660\n",
            "Iter 450: Loss 0.2374\n",
            "Iter 500: Loss 0.3532\n",
            "Iter 550: Loss 0.3237\n",
            "Iter 600: Loss 0.2649\n",
            "Iter 650: Loss 0.2583\n",
            "Iter 700: Loss 0.3090\n",
            "Iter 750: Loss 0.3050\n",
            "Iter 800: Loss 0.2617\n",
            "Iter 850: Loss 0.2687\n",
            "Iter 900: Loss 0.2259\n",
            "Iter 950: Loss 0.2957\n",
            "üî¥ EPOCH 13 DONE. Val PSNR: 26.0769 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_13.pth\n",
            "\n",
            "--- Reliable Epoch 14/20 ---\n",
            "Iter 0: Loss 0.2415\n",
            "Iter 50: Loss 0.4497\n",
            "Iter 100: Loss 0.2740\n",
            "Iter 150: Loss 0.2332\n",
            "Iter 200: Loss 0.3028\n",
            "Iter 250: Loss 0.2371\n",
            "Iter 300: Loss 0.2645\n",
            "Iter 350: Loss 0.2167\n",
            "Iter 400: Loss 0.3311\n",
            "Iter 450: Loss 0.2338\n",
            "Iter 500: Loss 0.2257\n",
            "Iter 550: Loss 0.3822\n",
            "Iter 600: Loss 0.2970\n",
            "Iter 650: Loss 0.2501\n",
            "Iter 700: Loss 0.2560\n",
            "Iter 750: Loss 0.3179\n",
            "Iter 800: Loss 0.3113\n",
            "Iter 850: Loss 0.3225\n",
            "Iter 900: Loss 0.2650\n",
            "Iter 950: Loss 0.3251\n",
            "üî¥ EPOCH 14 DONE. Val PSNR: 26.1025 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_14.pth\n",
            "\n",
            "--- Reliable Epoch 15/20 ---\n",
            "Iter 0: Loss 0.2629\n",
            "Iter 50: Loss 0.2601\n",
            "Iter 100: Loss 0.2415\n",
            "Iter 150: Loss 0.3208\n",
            "Iter 200: Loss 0.2638\n",
            "Iter 250: Loss 0.2821\n",
            "Iter 300: Loss 0.3534\n",
            "Iter 350: Loss 0.2593\n",
            "Iter 400: Loss 0.2556\n",
            "Iter 450: Loss 0.2605\n",
            "Iter 500: Loss 0.2305\n",
            "Iter 550: Loss 0.2795\n",
            "Iter 600: Loss 0.2996\n",
            "Iter 650: Loss 0.2199\n",
            "Iter 700: Loss 0.2870\n",
            "Iter 750: Loss 0.2512\n",
            "Iter 800: Loss 0.3066\n",
            "Iter 850: Loss 0.2216\n",
            "Iter 900: Loss 0.3090\n",
            "Iter 950: Loss 0.2771\n",
            "üî¥ EPOCH 15 DONE. Val PSNR: 26.0929 dB\n",
            "üíæ Saved: /content/drive/MyDrive/Shadow removal/model_refined_epoch_15.pth\n",
            "\n",
            "--- Reliable Epoch 16/20 ---\n",
            "Iter 0: Loss 0.2656\n",
            "Iter 50: Loss 0.2914\n",
            "Iter 100: Loss 0.2538\n",
            "Iter 150: Loss 0.2698\n",
            "Iter 200: Loss 0.3385\n",
            "Iter 250: Loss 0.3277\n",
            "Iter 300: Loss 0.2495\n",
            "Iter 350: Loss 0.2106\n",
            "Iter 400: Loss 0.4172\n",
            "Iter 450: Loss 0.3885\n",
            "Iter 500: Loss 0.2305\n",
            "Iter 550: Loss 0.2442\n",
            "Iter 600: Loss 0.2191\n",
            "Iter 650: Loss 0.2977\n",
            "Iter 700: Loss 0.2999\n",
            "Iter 750: Loss 0.2529\n",
            "Iter 800: Loss 0.2992\n",
            "Iter 850: Loss 0.3524\n",
            "Iter 900: Loss 0.4134\n",
            "Iter 950: Loss 0.2706\n"
          ]
        }
      ]
    }
  ]
}
